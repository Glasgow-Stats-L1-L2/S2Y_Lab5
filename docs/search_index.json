[["index.html", "1 Welcome to S2Y Lab 5 1.1 Introduction", " S2Y Lab 5 Further examination on model fit and assumptions - ANOVA, residual plots and transformations 1 Welcome to S2Y Lab 5 Intended Learning Outcomes: use R to fit linear models with transformed variables; obtain the elements of an analysis of variance (ANOVA) table; use the ANOVA table to compute and interpret the \\(F\\)-statistic and its hypothesis test. 1.1 Introduction In the lectures we studied analysis of variance (ANOVA) table which can be used to investigate the variability explained by the model and to test the null hypothesis that the parameters in the model of interest are zero: H\\(_0\\): all \\(p-1\\) parameters = 0 versus H\\(_1\\): at least one parameter \\(\\neq\\) 0 Component DF Sum Sq (SS) Mean Sq (MS) F Model \\(p-1\\) MSS MSS/(\\(p-1\\)) \\(\\frac{\\text{MSS}/(p-1)}{\\text{RSS}/(n-p)}\\) Residual \\(n-p\\) RSS RSS/(\\(n-p\\)) Total \\(n-1\\) TSS We also learned to apply logarithmic transformation to both response and predictor variables following scientific guidance. In today’s lab we will first revisit least squares estimate, the coefficient of determination (\\(R^2\\)) and the analysis of variance (ANOVA) table to deepen understanding of the R output. Then we will consider more types of transformations, to the response and/or predictors, and see how they help improve the model fit and correct violations of model assumptions. "],["example-1-log-transformation-on-x.html", "2 Example 1: log-transformation on \\(x\\) 2.1 Exploratory analysis 2.2 Statistical analysis 2.3 Assumption checking", " 2 Example 1: log-transformation on \\(x\\) Rossman (1994) collected information on life expectancy in various countries of the world and the densities of people per television set and of people per physician in those countries. The data is available in the LifeExp.csv file. In this lab, our focus is to identify how female life expectancy (\\(Y\\), abbreviated to FLE) is related to the number of people per physician (\\(x\\), abbreviated to PPP). 2.1 Exploratory analysis Firstly, produce a scatterplot of FLE against PPP by typing: life &lt;- read.csv(&quot;LifeExp.csv&quot;) plot(FLE ~ PPP, data = life) Figure 2.1: Scatterplot of female life expectancy versus the number of people per physician. DISCUSSION: Looking at the plot, what can we say about the relationship between female life expectancy and the number of people per physician? As the relationship appears to be non-linear, we apply log-transformation to the predictor variable. Transforming the values of \\(x\\) might be the first thing to try if there is a non-linear monotonic (i.e. entirely non-increasing or entirely non-decreasing) trend in the data, and non-linearity is the only problem. In other words, model assumptions (i.e. independence, zero-mean, constant variance, normality) should be met. The scatterplot of female life expectancy against \\(\\log\\)(PPP) is produced by typing: plot(FLE ~ log(PPP), data = life, xlab = &quot;log(PPP)&quot;) Figure 2.2: Scatterplot of female life expectancy versus log(number of people per physician). Figure 2.2 shows a linear trend after transforming \\(x\\). Therefore, we will model the relationship between FLE and \\(\\log\\)(PPP) using a simple linear regression model. The model is given as: \\[Y_i = \\alpha + \\beta \\log(x_i) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2), \\quad i = 1,\\ldots, 37.\\] 2.2 Statistical analysis A simple linear regression model can be fitted by using the lm command: Model1 &lt;- lm(FLE ~ log(PPP), data = life) The summary table and ANOVA table of this model can be produced by using summary() and anova(): summary(Model1) ## ## Call: ## lm(formula = FLE ~ log(PPP), data = life) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.065 -3.489 1.143 2.663 7.674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 109.9498 3.6860 29.83 &lt; 2e-16 *** ## log(PPP) -5.4893 0.5036 -10.90 8.48e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.286 on 35 degrees of freedom ## Multiple R-squared: 0.7724, Adjusted R-squared: 0.7659 ## F-statistic: 118.8 on 1 and 35 DF, p-value: 8.484e-13 anova(Model1) ## Analysis of Variance Table ## ## Response: FLE ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(PPP) 1 2182.34 2182.34 118.81 8.484e-13 *** ## Residuals 35 642.91 18.37 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 QUESTION: Using the following summary statistics, calculate the least squares estimates of \\(\\alpha\\) and \\(\\beta\\). Check your answers with the column of Estimate in the summary table. \\[\\bar{x} = 7.18,\\ \\bar{y} = 70.51,\\ S_{xx}=72.42,\\ S_{yy}=2825.24,\\ S_{xy}=-397.56\\] Hint \\[\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x},\\ \\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\\] Write down the equation of the fitted model. Based on it, comment on the least squares estimate of \\(\\beta\\) and predict the value of female life expectancy when the number of people per physician equals 4000. Solution The regression equation is \\[\\text{FLE} = 109.95 - 5.49 \\cdot \\log(\\text{PPP})\\] \\(\\hat{\\beta}\\) can be interpreted as: if \\(\\log(\\text{PPP})\\) increases by 1 unit, the expected female life expectancy decreases by 5.49. When predicting the value of the response for a new observation, we need to back transform the variable. For example, if the number of people per physician is 4000, the best prediction of female life expectancy is \\(109.95 - 5.49 \\cdot \\log(4000) \\approx 64.42\\). Use the summary statistics in (a) to complete the analysis of variance table below, i.e. finding the degrees of freedom, the sum of squares, the mean squared error, and the \\(F\\)-statistic. Check your answer with the anova output. Component DF Sum Sq (SS) Mean Sq (MS) F Model Residual Total What hypotheses are being examined by the \\(F\\)-statistic in the ANOVA table? By looking at its \\(p\\)-value, what can you conclude about the fitted model? Solution The null and alternative hypotheses are: H\\(_0\\): \\(\\beta=0\\) versus H\\(_1\\): \\(\\beta \\neq 0\\) Note that \\(\\alpha\\) is not included in the hypothesis. Since the \\(p\\)-value (from Pr(&gt;F)) is less than 0.05, we reject the null hypothesis and conclude that \\(\\log\\)(PPP) is useful in predicting FLE. Compute and interpret the coefficient of determination, \\(R^2\\). Check your answer with the term Multiple R-squared in the summary table. Hint \\[R^2 = 1-\\frac{\\text{RSS}}{\\text{TSS}}\\] Solution \\[\\begin{equation*} \\begin{split} R^2 &amp;= 1-\\frac{\\text{RSS}}{\\text{TSS}}\\\\ &amp;=1-\\frac{642.91}{2182.34+642.91}\\\\ &amp;=0.772 \\end{split} \\end{equation*}\\] The \\(R^2\\) tells us the model provides a relatively adequate fit to the data with 77% of variability in FLE can be explained by this model. Comment on the strength of linear relationship between \\(\\log\\)(PPP) and FLE. Hint The strength of a linear relationship can be quantified using the sample correlation coefficient. Solution We could calculate the sample correlation coefficient \\(r\\) based on its definition or use the fact that \\(r^2 = R^2\\) in the case of simple linear regression (see Chapter 2, \\(\\S\\) 2.1) Approach 1: based on the definition \\[\\begin{equation*} \\begin{split} r &amp;=\\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\\\ &amp;=\\frac{-397.56}{72.42\\cdot 2825.24}\\\\ &amp;=-0.879 \\end{split} \\end{equation*}\\] Approach 2: use \\(r^2=R^2\\) \\[|r|=\\sqrt{R^2} = \\sqrt{0.772}=0.879\\] Based on the scatterplot, we know that the relationship between \\(\\log\\)(PPP) and FLE is negative, and thus \\(r=-0.879\\). Approach 3: use the command cor in R cor(log(life$PPP),life$FLE) ## [1] -0.8788868 The sample correlation coefficient of \\(-0.879\\) tells us that there is a strong negative relationship between \\(\\log\\)(PPP) and FLE. 2.3 Assumption checking As seen in Lab 4, model assumptions can be assessed graphically by producing a plot of the residuals versus the fitted values and a normal probability plot (Q-Q plot) of the residuals with the following R commands: plot(rstandard(Model1) ~ fitted(Model1)) abline(h=0, lty=3, lwd=1.5) Figure 2.3: Standardised residuals versus fitted values plot from fitting a simple linear regression model to transformed variables. qqnorm(rstandard(Model1)) qqline(rstandard(Model1)) Figure 2.4: Normal probability (Q-Q) plot from fitting a simple linear regression model to transformed variables. Figure 2.3 displays the residual plots after fitting a simple linear regression model to the transformed variables. The points are fairly evenly scattered above and below the zero line, which suggests it is reasonable to assume that the random errors have mean zero. The vertical variation of the points seems to be small for small fitted values. However, there are also less points in this case. It would be preferable if more data are available. In the normal probability plot (Figure 2.4), we see that points do not exactly lie on diagonal line. This indicates that the normality assumption may not necessarily be satisfied. The independence of the random errors seems to be reasonable since each point refers to a different country. "],["example-2-power-transformation-on-y.html", "3 Example 2: power transformation on \\(y\\) 3.1 Exploratory analysis 3.2 Assumption checking", " 3 Example 2: power transformation on \\(y\\) The stopping.csv file contains 63 observations of cars. In these observations, two variables were recorded, namely the speed of cars when the brakes were applied (in mile per hour) and the stopping distance (in feet). Our question of interest is determine if there is a relationship between the speed of cars and the distance taken to stop. 3.1 Exploratory analysis The scatterplot of distance (\\(Y\\)) against the speed (\\(x\\)) in Figure 3.1 (left) shows that the variables do not appear to be linearly related. A possible remedy is to transform \\(Y\\) into \\(\\sqrt{Y}\\). A new scatterplot, Figure 3.1 (right), is obtained by using the command: Figure 3.1: Left: scatterplot of Distance versus Speed. Right: scatterplot of square root of Distance versus Speed. As the relationship now appears to be linear, we will build a simple linear regression model between \\(\\sqrt{\\text{distance}}\\) (as new \\(Y\\)) and speed as \\[Y_i = \\alpha + \\beta x_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2), \\quad i = 1,\\ldots, 63,\\] which can be fitted by using the command: Model2 &lt;- lm(sqrt(Distance) ~ Speed, data = stopping) 3.2 Assumption checking Figure 3.2 gives the residual plots after fitting a simple linear regression model to the original variables. Figure 3.2: Residual plots from fitting a simple linear regression model to original variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot. The plots show the problems of curvature, non-constant variance and non-normality, indicating that the wrong type of model was used. Figure 3.3 gives the residual plots after fitting a simple linear regression model to the transformed variables. Figure 3.3: Residual plots from fitting a simple linear regression model to transformed variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot. The curvature disappears and the variance is almost constant across the range of fitted values. The normality assumption, however, remains to be invalid. This is not ideal but, on the positive side, the estimates of parameters will not be affected and hence we can still use the model to describe the relationship between variables and make predictions. 3.2.1 Regression output summary(Model2) ## ## Call: ## lm(formula = sqrt(Distance) ~ Speed, data = stopping) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4879 -0.5487 0.0098 0.5291 1.5545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.918283 0.197406 4.652 1.82e-05 *** ## Speed 0.252568 0.009246 27.317 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7193 on 61 degrees of freedom ## Multiple R-squared: 0.9244, Adjusted R-squared: 0.9232 ## F-statistic: 746.2 on 1 and 61 DF, p-value: &lt; 2.2e-16 QUESTION: Write down the equation of the fitted model. Solution The regression equation is \\[\\sqrt{\\text{Distance}} = 0.918 + 0.253 \\cdot \\text{Speed} \\] Based on the regression equation in (a), comment on the relationship between speed and square root of distance. In addition, pick a speed value yourself and predict the distance for this speed. Solution The estimated parameter of \\(0.253\\) suggests the square root of distance is positively linearly related to speed. As the speed increases by 1 MPH, the expected square root of distance increases by 0.253 feet. When predicting the value of the response, we back transform the variable as \\(\\text{Distance} = (0.918 + 0.253 \\cdot \\text{Speed})^2\\). For example, if the speed is 20 MPH, the predicted distance is \\((0.918+0.252\\cdot 20)^2 \\approx 35.64\\) feet. Note that our model is built only for speed ranging from 4 to 40. It would be unwise to make predictions outside this range in the absence of other information. "],["optional-activity-more-on-log-transformation.html", "4 Optional activity: more on log-transformation 4.1 Log-transformation on \\(y\\) 4.2 log-transformation on both \\(x\\) and \\(y\\)", " 4 Optional activity: more on log-transformation The following two datasets may be explored to understand the effect of applying log-transformation on \\(y\\) only and on both \\(x\\) and \\(y\\). Remark: Identifying the appropriate transformation to address violation of assumptions is beyond the scope of this course. 4.1 Log-transformation on \\(y\\) Log-transforming the response variable may be considered when the assumptions of constant variances and/or non-normality are violated. As an added bonus, transforming the response variable may also help mitigate the problem of a curved relationship. To understand the effect of log-transforming \\(y\\), we look at a (fictional) salary data from the (fictional) company Initech. Data: initech.csv Columns:               C1: years of experience               C2: salary TASK 1 Produce a scatterplot of salary versus years. What can we say about this relationship? Fit a simple linear regression model to predict salary from year. Interpret the estimated parameters. Produce residual plots and comment on the assumptions for the simple linear regression model. Repeat the above using \\(\\log\\)(salary) and compare the residual plots. Using the model with the transformed response \\(\\log(Y)\\), predict the salary for someone with 10 years of experience. Hint After back transformation, \\(Y=e^{\\alpha} e^{\\beta x}\\). 4.2 log-transformation on both \\(x\\) and \\(y\\) Log-transforming both the response variable and the predictor variable may be useful in the scenario that the relationship is non-linear and the errors are not normal and have unequal variances. Source: Allison, T., &amp; Cicchetti, D. V. (1976). Sleep in mammals: ecological and constitutional correlates. Science, 194(4266), 732-734. Context: The original data were used for analysing the relationship between constitutional and ecological factors and sleeping (dreaming and non-dreaming) in mammals. Constitutional variables such as life span, body weight, brain weight and gestation time were evaluated. Ecological variables such as severity of predation, safety of sleeping place and overall danger were inferred from field observations in the literature. We are interested in finding out if how body weight and brain weight are related. Data: mammals.csv Columns:               C1: body_wt years of experience               C2: brain_wt salary TASK 2 Produce scatterplots of body weight versus brain weight, using both original variables and log-transformed variables. Which pair seems to have a linear relationship? Fit a simple linear regression model to describe the relationship between \\(\\log\\)(body weight) and \\(\\log\\)(brain weight). Examine the fit of the model using the coefficient of determination, \\(R^2\\), as well as residual plots. Interpret the estimated parameters and use them to estimate the mean body weight with brain weight of your choice. "],["solution.html", "5 Solution 5.1 Task 1 5.2 Task 2", " 5 Solution 5.1 Task 1 Read in the data and produce the scatterplots: initech &lt;- read.csv(&quot;initech.csv&quot;) plot(salary ~ years, data = initech) plot(log(salary) ~ years, data = initech) Statistical analysis: Model3 &lt;- lm(salary ~ years, data = initech) summary(Model3) Model4 &lt;- lm(log(salary) ~ years, data = initech) summary(Model4) Residual plots and assumption checking: plot(rstandard(Model3) ~ fitted(Model3), xlab = &quot;Fitted values&quot;, ylab = &quot;Standardised residuals&quot;) abline(h=0, lty=3, lwd=1.5) qqnorm(rstandard(Model3)) qqline(rstandard(Model3)) plot(rstandard(Model4) ~ fitted(Model4), xlab = &quot;Fitted values&quot;, ylab = &quot;Standardised residuals&quot;) abline(h=0, lty=3, lwd=1.5) qqnorm(rstandard(Model4)) qqline(rstandard(Model4)) The regression equation with years of experience as the predictor variable and log-transformed salary as the response variable is given by: \\[\\log(\\text{Salary})=10.48 + 0.079 \\cdot \\text{Years}\\] For an individual with 10 years of experience, the predicted value of salary is \\(\\exp(10.48)\\cdot \\exp(0.079\\cdot 10)\\approx 78433\\). 5.2 Task 2 Read in the data and produce the scatterplots: mammals &lt;- read.csv(&quot;mammals.csv&quot;) plot(body_wt ~ brain_wt, data = mammals) plot(body_wt ~ log(brain_wt), data = mammals) plot(log(body_wt) ~ log(brain_wt), data = mammals) For this particular dataset, it doesn't matter whether the body weight is the response or the brain weight is the response. The analysis below is performed assuming body weight is the response. Model5 &lt;- lm(log(body_wt) ~ log(brain_wt), data = mammals) summary(Model5) Residual plots and assumption checking: plot(rstandard(Model5) ~ fitted(Model5), xlab = &quot;Fitted values&quot;, ylab = &quot;Standardised residuals&quot;) abline(h=0, lty=3,lwd=1.5) qqnorm(rstandard(Model5)) qqline(rstandard(Model5)) The regression equation with \\(\\log\\)(body weight) as the predictor variable and log-transformed salary as the response variable is given by: \\[\\log(\\text{body}\\_\\text{wt})=-2.51 + 1.22 \\cdot \\log(\\text{body}\\_\\text{wt})\\] When predicting the value of body weight, we back transform the variable as \\(\\text{body}\\_\\text{wt} = \\exp(-2.51)\\cdot\\text{body}\\_\\text{wt}^{1.22}\\). For example, for a mammal with brain weight of 20, the predicted body weight is \\(\\exp(-2.51)\\cdot20^{1.22}\\approx 3.14\\). Note that your choice of brain weight should be within the range of 0.14 and 5712, i.e. the minimum and maximum value of brain weight in this dataset. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
